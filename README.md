# Airtable Contractor Pipeline ‚Äî Technical Documentation

_Last updated: 2025‚Äë09‚Äë05_

## Overview
This document explains how to set up, operate, and extend the **Contractor Application Pipeline** that connects Airtable with local Python scripts to:

- Collect multi‚Äëtable applicant data via forms
- **Compress** the normalized records into a single JSON blob on the parent row
- **Decompress** that JSON back into child tables for editing
- **Shortlist** candidates based on multi‚Äëfactor rules
- Run an **LLM evaluation** to summarize, score, and flag issues

The doc includes schema definitions, setup steps, CLI usage, code walkthroughs, and guidance to customize rules and prompts.

---

## Table of Contents
1. [Airtable Base & Schema](#airtable-base--schema)
2. [Environment & Local Setup](#environment--local-setup)
3. [Form Flow (Multi‚Äëtable collection)](#form-flow-multi-table-collection)
4. [Pipeline CLI & Modes](#pipeline-cli--modes)
5. [Compression & Decompression](#compression--decompression)
6. [Shortlisting Rules & Automation](#shortlisting-rules--automation)
7. [LLM Evaluation (Groq/OpenAI/Gemini)](#llm-evaluation-groqopenaigemini)
8. [Security & Budget Guardrails](#security--budget-guardrails)
9. [Extending & Customizing](#extending--customizing)
10. [Troubleshooting & Runbook](#troubleshooting--runbook)
11. [Known Gaps & Suggested Fixes](#known-gaps--suggested-fixes)
12. [Appendix: Config Template](#appendix-config-template)

---

## Airtable Base & Schema

### Tables

1) **Applicants** (parent)
- **Primary field**: `Applicant ID` (single line text or autogenerated)
- **Fields**:
  - `Compressed JSON` (Long text)
  - `Shortlist Status` (Single select: values `yes`, `no` or status options like `Not evaluated`, `Shortlisted`, `Ineligible`)
  - `LLM Summary` (Long text)
  - `LLM Score` (Number)
  - `LLM Follow-Ups` (Long text ‚Äì newline‚Äëseparated)
  - `LLM Data Hash` (Single line text ‚Äì stores hash of last evaluated JSON)

2) **Personal Details** (child, 1‚Äëto‚Äë1)
- **Fields**:
  - `Full Name`
  - `Email`
  - `Location`
  - `LinkedIn`
  - **Link to Applicants** ‚Üí field name must match `LINK_FIELD` in code (default suggested: `Applicant`)

3) **Work Experience** (child, 1‚Äëto‚Äëmany)
- **Fields**:
  - `Company`
  - `Title`
  - `Start` (Date)
  - `End` (Date, optional)
  - `Technologies`
  - **Link to Applicants** ‚Üí `Applicant`

4) **Salary Preferences** (child, 1‚Äëto‚Äë1)
- **Fields**:
  - `Preferred Rate` (Number)
  - `Minimum Rate` (Number)
  - `Currency` (Single select or text, e.g., `USD`)
  - `Availability (hrs/wk)` (Number)
  - **Link to Applicants** ‚Üí `Applicant`

5) **Shortlisted Leads** (helper)
- **Fields**:
  - **Link to Applicants** ‚Üí field name must match `SHORTLIST_LINK_FIELD` in code (suggested: `Applicant`)
  - `Compressed JSON` (Long text)
  - `Score Reason` (Long text)
  - `Created At` (Created time ‚Äì Airtable auto)

> üîó **Important**: In Airtable, the linked-record field name used in child tables **must** match the constant `LINK_FIELD` in your code. Likewise the link on **Shortlisted Leads** must match `SHORTLIST_LINK_FIELD`.

---

## Environment & Local Setup

1) **Clone** the repository containing the provided Python modules:
```
processors/
  compressor.py
  decompressor.py
  shortlister.py
  llm_evaluator.py
utils/
  airtable_client.py
  helpers.py
config.py
contractor_pipeline.py
manual_tools.py
```

2) **Create & fill** a `.env` (or system env vars) and a `config.py`:
- See [Appendix](#appendix-config-template) for a template.
- Key values: `AIRTABLE_TOKEN`, `BASE_ID`, table names, link field names, shortlisting constants (`MIN_EXPERIENCE_YEARS`, `MAX_HOURLY_RATE`, `MIN_AVAILABILITY`, `ALLOWED_LOCATIONS`, `TIER1_COMPANIES`), and LLM settings (`GROQ_API_KEY`, `LLM_MODEL`, `MAX_TOKENS`).

3) **Install dependencies**:
```bash
pip install pyairtable python-dateutil groq pydantic
```

4) **Sanity test** Airtable connection (reads without error):
```python
from utils.airtable_client import airtable
print(airtable.get_all_applicants()[:1])
```

---

## Form Flow (Multi‚Äëtable collection)
Airtable forms write to **one table at a time**. Use one form per child table:

- **Form A ‚Äì Personal Details** ‚Üí creates a row in `Personal Details` linked to `Applicants` by asking for or pre‚Äëfilling `Applicant ID`.
- **Form B ‚Äì Work Experience** ‚Üí multiple submissions allowed; ensure the same `Applicant ID` is used.
- **Form C ‚Äì Salary Preferences** ‚Üí single row linked to the same `Applicant ID`.

**Prefill URL pattern** (example):
```
https://airtable.com/<formId>?prefill_Applicant=<ApplicantRecordId>
```
Require candidates to submit all three forms. The pipeline scripts read these linked rows.

---

## Pipeline CLI & Modes

### Main pipeline
```bash
python contractor_pipeline.py --mode new_only|changed|all [--applicant <recId>] [--dry-run]
```
- `new_only`: process Applicants with empty `Compressed JSON`.
- `changed`: process Applicants where `Compressed JSON` exists but `LLM Summary` is empty (proxy for changed / not yet evaluated).
- `all`: re‚Äëcompress every Applicant, then re‚Äërun shortlisting and LLM.
- `--applicant`: focus on one record id.
- `--dry-run`: list what would be processed.

### Manual tools
```bash
python manual_tools.py decompress --applicant <recId>
python manual_tools.py reprocess  --applicant <recId>
python manual_tools.py view       --applicant <recId>
python manual_tools.py list       --limit 10
```
- **decompress**: delete existing child rows, recreate from JSON, so you can edit in the UI.
- **reprocess**: recompress + re‚Äëevaluate with LLM.
- **view**: human‚Äëreadable summary across stages.
- **list**: recent Applicants with status indicators.

---

## Compression & Decompression

### Compression (`processors/compressor.py`)
- Reads linked rows from `Personal Details`, `Work Experience`, `Salary Preferences` for each Applicant.
- Builds a single JSON payload:
```json
{
  "personal": {"name": "‚Ä¶", "email": "‚Ä¶", "location": "‚Ä¶", "linkedin": "‚Ä¶"},
  "experience": [ {"company": "‚Ä¶", "title": "‚Ä¶", "start": "‚Ä¶", "end": "‚Ä¶", "technologies": "‚Ä¶"}, ‚Ä¶ ],
  "salary": {"preferred_rate": 100, "minimum_rate": 90, "currency": "USD", "availability": 25}
}
```
- Writes the stringified JSON to `Applicants.Compressed JSON`.
- `compress_all_applicants()` respects **idempotency** by skipping rows that already have JSON (unless you use `--mode all`).
- Retries: uses `@retry_with_backoff()` wrapper for basic resilience.

**Key snippet:**
```python
compressed_json = json.dumps(json_data, ensure_ascii=False)
self.client.update_applicant(applicant_record_id, {"Compressed JSON": compressed_json})
```

### Decompression (`processors/decompressor.py`)
- Reads `Applicants.Compressed JSON`.
- **Deletes** existing child rows for that Applicant (clean slate).
- Recreates 1:1 Personal, N Work Experience, 1:1 Salary rows **linked back** using `LINK_FIELD`.

**Key snippet:**
```python
fields = {LINK_FIELD: [applicant_record_id]}
# ‚Ä¶ populate fields ‚Ä¶
self.client.experience.create(fields)
```

> ‚ÑπÔ∏è The delete‚Äëand‚Äërecreate approach is a pragmatic v1 for consistency. If you need audit trails, switch to an upsert model that diffs by (Company, Title, Start) or a generated row key.

---

## Shortlisting Rules & Automation

Module: `processors/shortlister.py`

**Criteria** (as configured via `config.py`):
- **Experience**: total years ‚â• `MIN_EXPERIENCE_YEARS` **OR** any **Tier‚Äë1** company (`TIER1_COMPANIES`).
- **Compensation**: `Currency == USD` **AND** `Preferred Rate ‚â§ MAX_HOURLY_RATE` **AND** `Availability ‚â• MIN_AVAILABILITY`.
- **Location**: string‚Äëmatch against any value in `ALLOWED_LOCATIONS` (case‚Äëinsensitive, substring match).

**Flow**
1. For each Applicant, gather linked rows.
2. Compute **total years** using `calculate_experience_years()` from normalized dates.
3. Check compensation and location rules.
4. If eligible, **create** a row in **Shortlisted Leads** with:
   - Link to the Applicant (`SHORTLIST_LINK_FIELD`: `[<ApplicantId>]`)
   - Copy of `Compressed JSON`
   - `Score Reason` (passed reasons joined with `\n`)
5. Update `Applicants.Shortlist Status = "yes"`.

**Key snippet:**
```python
shortlist_data = {
  SHORTLIST_LINK_FIELD: [str(applicant_record.get("id"))],
  "Compressed JSON": evaluation["compressed_json"],
  "Score Reason": "\n ".join(evaluation["reasons"]),
}
self.client.shortlisted.create(shortlist_data)
self.client.update_applicant(applicant_record["id"], {"Shortlist Status": "yes"})
```

> ‚úÖ The code correctly uses a **list** for linked record IDs (Airtable requirement).

---

## LLM Evaluation (Groq/OpenAI/Gemini)

Module: `processors/llm_evaluator.py`

**Trigger**: Typically run in **Phase 3** of the pipeline after compression, or on `reprocess`.

**Prompting**
- The pipeline builds a **JSON‚Äëstructured instruction** with roles, rules, output schema, and an inline example. This nudges the model to return a JSON object.
- The Groq client is called with `response_format={"type": "json_object"}` to further enforce JSON output.

**Expected JSON**
```json
{
  "summary": "string (<= 75 words)",
  "score": 1,
  "issues": ["‚Ä¶"],
  "follow_ups": ["‚Ä¶", "‚Ä¶"]
}
```

**Writes to Airtable**
- `LLM Summary` ‚Üê `summary`
- `LLM Score`   ‚Üê `score`
- `LLM Follow-Ups` ‚Üê newline‚Äëjoined list from `follow_ups`
- **Recommended**: also write `LLM Data Hash` to persist change detection (see [Known Gaps](#known-gaps--suggested-fixes)).

**Tokens & retries**
- Returns and aggregates `tokens_used` when available.
- Automatic retry with exponential backoff is applied at the method level via `@retry_with_backoff()`.

**Key snippet:**
```python
response = self.groq_client.chat.completions.create(
  model=self.model,
  messages=[{"role": "user", "content": prompt}],
  max_tokens=MAX_TOKENS,
  temperature=0.3,
  response_format={"type": "json_object"}
)
parsed_result = json.loads(response.choices[0].message.content)
self.client.update_applicant(record_id, {
  "LLM Summary": parsed_result["summary"],
  "LLM Score": parsed_result["score"],
  "LLM Follow-Ups": "\n".join(parsed_result["follow_ups"]),
})
```

**Fallback parsing**
- If the model ever returns non‚ÄëJSON, `_parse_llm_response()` tries to extract the four sections from labeled text (`Summary:`, `Score:`, ‚Ä¶). This is a defensive fallback.

---

## Security & Budget Guardrails

- **Secrets**: Never hard‚Äëcode. Load `AIRTABLE_TOKEN`, `GROQ_API_KEY` (or OpenAI/Gemini keys) from env/.env.
- **Scopes**: Airtable token should be limited to the specific base.
- **Token caps**: Control `MAX_TOKENS` in `config.py`. Keep `temperature` low for determinism.
- **Change detection**: Use an **MD5 hash** of `Compressed JSON` to skip unnecessary LLM calls.
- **Rate limiting**: A small `time.sleep(0.5)` between requests reduces burst risk.
- **Logging**: The pipeline prints phase summaries and per‚Äërecord results; you can swap in `logging` later.

---

## Extending & Customizing

### 1) Shortlist rules (LLM‚Äëassisted)
You can move more logic into the LLM while keeping **hard safety rails** in code. Example hybrid approach:
- **Code** enforces hard filters (min years, currency, max rate, availability, allowed geos).
- **LLM** provides a **contextual score** for pedigree, domain fit, and role alignment using a rubric.
- Store a combined score (e.g., `0.6*code_score + 0.4*llm_score`).

Example rubric prompt addition:
```json
{
  "instructions": {
    "task": [
      "‚Ä¶",
      "Additionally, assign a rubric_score (1-10) based on pedigree (Tier-1 employers or comparable), role alignment to full-stack SWE, and evidence of ownership (projects shipped, leadership)."
    ],
    "output_schema": {
      "summary": "string",
      "score": "integer (1-10)",
      "rubric_score": "integer (1-10)",
      "issues": ["string"],
      "follow_ups": ["string"]
    }
  }
}
```
Then combine in Python before shortlisting.

### 2) Currency handling
- Add a helper to convert non‚ÄëUSD rates to USD via a daily FX rate. Cache FX in a table `FX Rates` (Date, USD/EUR/INR‚Ä¶).
- Update `_evaluate_compensation()` to convert `preferred_rate` to USD before comparing to `MAX_HOURLY_RATE`.

### 3) Location normalization
- Maintain a `Locations` helper table mapping free‚Äëtext ‚Üí (Country, RegionCode). Use exact country checks instead of substring.

### 4) Tier‚Äë1 companies table
- Replace hardcoded `TIER1_COMPANIES` with a `Tier1 Companies` table so ops can update without code changes.

### 5) Idempotent upserts for experience
- Rather than delete‚Äëand‚Äërecreate, upsert by stable key (e.g., `hash(Company|Title|Start)` stored in a hidden field). Update changed rows and remove missing ones.

### 6) Webhook/Automation trigger
- Add an Airtable Automation to POST to a local endpoint or invoke GitHub Actions/Cloud Run to trigger the pipeline when any child table row changes.

---

## Troubleshooting & Runbook

**Forms created but compression skips**
- Ensure child rows are **linked** to the Applicant via the `Applicant` field matching `LINK_FIELD`.

**Shortlisting never triggers**
- Check `Currency == USD`, `Preferred Rate` numeric, and `Availability` numeric.
- Confirm `ALLOWED_LOCATIONS` and your `Location` strings line up (lowercase/substring right now).

**LLM results not updating**
- Look for JSON parsing failures; check the console output of `_parse_llm_response()`.
- Ensure `GROQ_API_KEY` is correct and `LLM_MODEL` is available to your account.

**Experience years look wrong**
- Validate `Start`/`End` dates are true date fields; empty `End` defaults to **today**.

**API 422 from Airtable**
- When writing arrays (links, multi‚Äëselect), ensure **lists** are used. E.g., `SHORTLIST_LINK_FIELD: ["recXXXX"]`.

---

## Known Gaps & Suggested Fixes

1) **Persist `LLM Data Hash`**
- The evaluator computes `current_hash` and reads `LLM Data Hash`, but does **not** save it after a successful run. Add:
```python
self.client.update_applicant(record_id, {
  "LLM Summary": parsed_result["summary"],
  "LLM Score": parsed_result["score"],
  "LLM Follow-Ups": "\n".join(parsed_result["follow_ups"]),
  "LLM Data Hash": self._get_json_hash(json_data),
})
```

2) **Shortlist status values**
- If you use a single‚Äëselect, align values with code (`"yes"`) or switch to a checkbox boolean and update accordingly.

3) **Linked record lookups**
- `linked_records()` pulls **all** rows then filters in Python. For large bases, replace with formula filters (`pyairtable.Table.all(formula=‚Ä¶)`) to query by link.

4) **Rate normalization**
- Currently requires `USD`. If you intake global candidates, add FX conversion.

5) **Logging**
- Replace `print()` with `logging` and structured fields (record id, phase, duration) for easier ops.

---

## Appendix: Config Template

Create `config.py` (values shown are **examples**):

```python
import os

AIRTABLE_TOKEN = os.getenv("AIRTABLE_TOKEN")
BASE_ID = os.getenv("AIRTABLE_BASE_ID")

# Table names
T_APPLICANTS = "Applicants"
T_PERSONAL = "Personal Details"
T_EXPERIENCE = "Work Experience"
T_SALARY = "Salary Preferences"
T_SHORTLISTED = "Shortlisted Leads"

# Linked field names (must match Airtable schema)
LINK_FIELD = "Applicant"                 # used in child tables
SHORTLIST_LINK_FIELD = "Applicant"       # used in Shortlisted Leads

# Shortlisting constants
MIN_EXPERIENCE_YEARS = 4
MAX_HOURLY_RATE = 100
MIN_AVAILABILITY = 20
ALLOWED_LOCATIONS = ["united states", "canada", "united kingdom", "germany", "india"]
TIER1_COMPANIES = ["google", "meta", "openai", "apple", "microsoft", "amazon", "nvidia"]

# LLM
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
LLM_MODEL = os.getenv("LLM_MODEL", "llama-3.1-70b-versatile")
MAX_TOKENS = int(os.getenv("MAX_TOKENS", "400"))
```

**.env example**
```
AIRTABLE_TOKEN=pat_********
AIRTABLE_BASE_ID=app********
GROQ_API_KEY=gsk_********
LLM_MODEL=llama-3.1-70b-versatile
MAX_TOKENS=400
```

---

## Code Walkthrough (Deep Dive)

### 1) `utils/airtable_client.py`
- Wraps `pyairtable.Api` and exposes typed table handles.
- Helper methods:
  - `get_all_applicants()` ‚Üí list of records
  - `get_applicant(record_id)` ‚Üí single record
  - `update_applicant(record_id, fields)` ‚Üí patch
  - `linked_records(table, applicant_rec_id)` ‚Üí **current** implementation fetches all rows and filters by `LINK_FIELD`. Consider formula filtering for scale.

### 2) `utils/helpers.py`
- `retry_with_backoff` decorator for transient errors.
- `safe_get_field(record, name, default)` defensive accessor.
- `calculate_experience_years(exp_records)` sums day deltas across roles; empty `End` ‚Üí today.

### 3) `processors/compressor.py`
- Builds the canonical compressed JSON from normalized child rows. Ensures only populated fields are included.
- `compress_all_applicants()` skips rows with existing JSON (v1 idempotency).

### 4) `processors/decompressor.py`
- Deletes linked child rows and recreates them from the JSON snapshot. Guarantees UI reflects JSON state.

### 5) `processors/shortlister.py`
- Computes experience years and Tier‚Äë1 match; compensation and location checks; writes Shortlisted Lead, updates status.
- Prints **selected** / **rejected** with reason strings for traceability.

### 6) `processors/llm_evaluator.py`
- Builds a JSON‚Äëfirst instruction, calls Groq Chat Completions, enforces JSON response.
- Writes back summary/score/follow‚Äëups, returns token usage when available.
- Skips repeated evaluation if `LLM Summary` exists **and** `LLM Data Hash` equals the current JSON hash (add the write‚Äëback fix in [Known Gaps](#known-gaps--suggested-fixes)).

### 7) Entrypoints
- `contractor_pipeline.py` orchestrates **three phases** with colored/emoji logs and a final summary. Supports `--mode`, `--applicant`, `--dry-run`.
- `manual_tools.py` offers surgical operations for editing and inspection.

---

## What Next
- Decide whether to switch from **delete‚Äëand‚Äërecreate** to **upsert** for child tables.
- Externalize Tier‚Äë1 and Location logic into helper tables for ops control.
- Add a simple **unit test** layer for the experience calculator and compensation validator.
- Add a **Makefile** or small shell script to run common commands (compress, shortlist, evaluate, full pipeline).

---

**End of Document**
